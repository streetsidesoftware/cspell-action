"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.createParser = exports.parseDocument = void 0;
const sync_1 = require("@cspell/cspell-pipe/sync");
const tokenizeLine_js_1 = require("./tokenizeLine.js");
function parseDocument(grammar, _filename, content, emitter = (line) => console.log(line)) {
    const r = (0, tokenizeLine_js_1.tokenizeText)(content, grammar);
    const tokens = (0, sync_1.pipe)(r, (0, sync_1.opMap)((tl) => tl.tokens.map((t) => ({ t, l: tl.line }))), (0, sync_1.opFlatten)(), (0, sync_1.opFilter)((t) => !t.t.scope.value.startsWith('punctuation')));
    for (const { t: token, l: line } of tokens) {
        emitter(`${(token.range[2] ?? line.lineNumber) + 1}:${token.range[0] + 1}\t ${JSON.stringify(token.text)}\t ${token.scope.toString()}`);
    }
}
exports.parseDocument = parseDocument;
function mapTokenizedLine(tl) {
    return tl.tokens.map((t) => ({
        text: t.text,
        range: [tl.offset + t.range[0], tl.offset + t.range[1]],
        scope: t.scope,
    }));
}
function mapTokenizedLines(itl) {
    return (0, sync_1.pipe)(itl, (0, sync_1.opMap)(mapTokenizedLine), (0, sync_1.opFlatten)());
}
function createParser(grammar, name, transform = mapTokenizedLines) {
    function parse(content, filename) {
        const parsedTexts = (0, sync_1.pipe)((0, tokenizeLine_js_1.tokenizeTextIterable)(content, grammar), transform);
        return { content, filename, parsedTexts };
    }
    return { name, parse };
}
exports.createParser = createParser;
//# sourceMappingURL=parser.js.map